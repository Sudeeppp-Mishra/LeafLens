{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMw/kgIMWJkVo5Hvh2wOiB0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sudeeppp-Mishra/LeafLens/blob/main/LeafLens_Train_on_DataSets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LeafLens: AI-Based Plant Disease Detection\n",
        "\n",
        "LeafLens classifies plant leaf images into healthy or diseased categories.\n",
        "We use a pretrained ResNet18 model (transfer learning) for accuracy and faster training.\n",
        "\n",
        "**Libraries used:**\n",
        "- PIL / OpenCV / NumPy: Image loading and preprocessing\n",
        "- PyTorch / Torchvision: Model training and evaluation\n",
        "- Matplotlib: Visualization of metrics\n",
        "- PySide6 / pyttsx3 / gTTS: GUI + audio output (later integration)\n",
        "\n",
        "**Dataset:** PlantVillage (Image Classification)  \n",
        "**Platform:** Google Colab (GPU)"
      ],
      "metadata": {
        "id": "QdoIfBU02_pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why GPU?\n",
        "\n",
        "Deep learning models require heavy computation.  \n",
        "Using GPU in Colab speeds up training significantly."
      ],
      "metadata": {
        "id": "dxngTJlc342E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive\n",
        "\n",
        "Accessing PlantVillage dataset stored in Google Drive.  \n",
        "This avoids repeated uploads and keeps large datasets organized."
      ],
      "metadata": {
        "id": "8owmyFw938td"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doSPHUp6jDVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47739e01-2a4e-4cee-ff50-fb4f2a43e217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required Libraries\n",
        "\n",
        "Purpose\n",
        "\n",
        "We import all required libraries for:\n",
        "\n",
        "\t•\tDeep learning (PyTorch)\n",
        "\t•\tImage processing (OpenCV, PIL)\n",
        "\t•\tDataset handling\n",
        "\t•\tVisualization\n",
        "\n",
        "We use:\n",
        "\n",
        "\t•\tOpenCV → fast, robust image reading\n",
        "\t•\tPIL → transformations compatibility\n",
        "\t•\tPyTorch → CNN training framework"
      ],
      "metadata": {
        "id": "lXoyeV3PG6Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchvision import transforms, models\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "n4rC4f0g4NkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check GPU\n",
        "\n",
        "We confirm if GPU is available.  \n",
        "Training on GPU is much faster than CPU."
      ],
      "metadata": {
        "id": "X3oLLce_HPsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6oTssV8G8z0",
        "outputId": "d2e8ac2e-087a-4ce6-f44f-83e72583adfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Path\n",
        "\n",
        "Why?\n",
        "\n",
        "We tell Python where PlantVillage dataset is located."
      ],
      "metadata": {
        "id": "IlByVJX8HfhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_DIR = \"/content/drive/MyDrive/datasets/PlantVillage\"\n",
        "print(\"Classes found:\", os.listdir(DATASET_DIR))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPsqNeDUHSW3",
        "outputId": "77029052-5dfb-410f-ef21-dc486a5c5f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes found: ['Tomato_healthy', 'PlantVillage', 'Tomato__Tomato_YellowLeaf__Curl_Virus', 'Tomato_Spider_mites_Two_spotted_spider_mite', 'Tomato_Leaf_Mold', 'Potato___Early_blight', 'Tomato_Septoria_leaf_spot', 'Potato___Late_blight', 'Tomato_Early_blight', 'Tomato__Target_Spot', 'Pepper__bell___healthy', 'Tomato__Tomato_mosaic_virus', 'Tomato_Bacterial_spot', 'Potato___healthy', 'Pepper__bell___Bacterial_spot', 'Tomato_Late_blight']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Image Paths and Labels\n",
        "\n",
        "Why??\n",
        "\n",
        "CNN does not understand folders.\n",
        "\n",
        "We convert:\n",
        "\n",
        "\t•\tFolder names → class labels\n",
        "\t•\tImages → file paths"
      ],
      "metadata": {
        "id": "uiPKumztHrJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = []\n",
        "labels = []\n",
        "class_names = sorted(os.listdir(DATASET_DIR))\n",
        "\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}\n",
        "\n",
        "for cls in class_names:\n",
        "    cls_path = os.path.join(DATASET_DIR, cls)\n",
        "    if not os.path.isdir(cls_path):\n",
        "        continue\n",
        "    for img in os.listdir(cls_path):\n",
        "        image_paths.append(os.path.join(cls_path, img))\n",
        "        labels.append(class_to_idx[cls])\n",
        "\n",
        "print(\"Total images:\", len(image_paths))\n",
        "print(\"Total classes:\", len(class_names))"
      ],
      "metadata": {
        "id": "UZ7vXcaZHiUD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f33ec9-e818-4c83-9071-1a2e696cff81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 20654\n",
            "Total classes: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training-Validation Split (80-20)\n",
        "\n",
        "WHY THIS IS MANDATORY\n",
        "\n",
        "Why not train on 100% data?\n",
        "\n",
        "Because:\n",
        "\t•\tModel may memorize images (overfitting)\n",
        "\t•\tYou cannot measure real performance\n",
        "\t•\tTeacher will ask: “How do you know it works on unseen data?”\n",
        "\n",
        "80% → learning\n",
        "\n",
        "20% → evaluation (unseen images)"
      ],
      "metadata": {
        "id": "CuTMzYUVIJE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "print(\"Training images:\", len(train_paths))\n",
        "print(\"Validation images:\", len(val_paths))"
      ],
      "metadata": {
        "id": "6_W-f3NzHt5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d83457a-3cd2-4df8-abeb-b187462669f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images: 16523\n",
            "Validation images: 4131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Preprocessing (OpenCV + PIL)\n",
        "\n",
        "Why preprocessing BEFORE training\n",
        "\n",
        "CNN expects:\n",
        "\n",
        "\t•\tSame image size\n",
        "\t•\tNormalized pixel values\n",
        "\t•\tClean data\n",
        "\n",
        "We use:\n",
        "\n",
        "\t•\tOpenCV → read image\n",
        "\t•\tPIL → apply transforms\n",
        "\t•\tNormalization → faster convergence\n"
      ],
      "metadata": {
        "id": "w1l9-tRhIlyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "tucq7E-eINHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAFE Dataset Class\n",
        "\n",
        "WHY THIS IS CRITICAL\n",
        "\n",
        "\t•\tSome images may be corrupted\n",
        "\t•\tOpenCV returns None\n",
        "\t•\tThis prevents infinite freeze"
      ],
      "metadata": {
        "id": "Fg9yvD7zIwYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SafePlantDataset(Dataset):\n",
        "    def __init__(self, paths, labels, transform=None):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.paths[idx]\n",
        "\n",
        "        try:\n",
        "            image = cv2.imread(path)\n",
        "            if image is None:\n",
        "                raise ValueError(\"Corrupted image\")\n",
        "\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            return image, self.labels[idx]\n",
        "\n",
        "        except:\n",
        "            return self.__getitem__((idx + 1) % len(self.paths))"
      ],
      "metadata": {
        "id": "0L6D34jhIorG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader\n",
        "\n",
        "- Loads batches efficiently  \n",
        "- Shuffles training data for better convergence"
      ],
      "metadata": {
        "id": "mScgoieXI499"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SafePlantDataset(train_paths, train_labels, train_transform)\n",
        "val_dataset = SafePlantDataset(val_paths, val_labels, val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "PnGv0tRNIynn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Selection - EfficientNet\n",
        "Why EfficientNet?\n",
        "\n",
        "\t•\tBetter accuracy than ResNet\n",
        "\t•\tFewer parameters\n",
        "\t•\tFaster on limited GPU"
      ],
      "metadata": {
        "id": "s1QNFk0kJRtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.efficientnet_b0(pretrained=True)\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, len(class_names))\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liBSZOUyI753",
        "outputId": "c0cbc774-225f-43e9-aa7d-caa1268ef517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function & Optimizer\n",
        "\n",
        "- CrossEntropyLoss for multi-class classification  \n",
        "- Adam optimizer adapts learning rate for faster convergence"
      ],
      "metadata": {
        "id": "dlw9KHh2Jd3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "y06e2wlgJURF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "- Forward pass → Compute loss → Backpropagate → Update weights  \n",
        "- Track training and validation accuracy  \n",
        "- GPU is used if available for faster computation"
      ],
      "metadata": {
        "id": "Ya8AIQkSJpYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing tqdm to see progress of training"
      ],
      "metadata": {
        "id": "oSD3MWQxTwQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "V12ClWoXJhSe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70ccbc1c-088e-4d39-8a81-af2b0948be75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Parameters\n",
        "epochs = 5\n",
        "checkpoint_dir = \"/content/drive/MyDrive/checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "save_every_n_batches = 100  # adjust for speed vs safety\n",
        "\n",
        "# OPTIONAL: resume from checkpoint\n",
        "start_epoch = 0\n",
        "start_batch = 0\n",
        "checkpoint_path = None  # set path if resuming\n",
        "\n",
        "if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "    print(f\"Resuming training from checkpoint: {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    start_batch = checkpoint['batch'] + 1\n",
        "    print(f\"Resuming from epoch {start_epoch+1}, batch {start_batch}\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\", leave=False)\n",
        "\n",
        "    for i, (images, labels) in progress_bar:\n",
        "        # Skip batches if resuming\n",
        "        if epoch == start_epoch and i < start_batch:\n",
        "            continue\n",
        "\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Save checkpoint every N batches\n",
        "        if (i + 1) % save_every_n_batches == 0:\n",
        "            checkpoint_file = os.path.join(checkpoint_dir, f\"checkpoint_epoch{epoch+1}_batch{i+1}.pth\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'batch': i,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict()\n",
        "            }, checkpoint_file)\n",
        "            print(f\"\\nCheckpoint saved: {checkpoint_file}\")\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Training Loss Epoch {epoch+1}: {avg_loss:.4f}\")\n",
        "\n",
        "    # Optional: save checkpoint at end of each epoch\n",
        "    epoch_checkpoint_file = os.path.join(checkpoint_dir, f\"checkpoint_epoch{epoch+1}_end.pth\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'batch': i,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, epoch_checkpoint_file)\n",
        "    print(f\"Epoch {epoch+1} checkpoint saved: {epoch_checkpoint_file}\")"
      ],
      "metadata": {
        "id": "vqB4V4jMM6t0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "261c958e-2d04-426b-9d46-872c8425ee38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  19%|█▉        | 100/517 [20:44<1:23:07, 11.96s/it, loss=0.734]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch1_batch100.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  39%|███▊      | 200/517 [41:04<1:04:06, 12.14s/it, loss=0.334]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch1_batch200.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  58%|█████▊    | 300/517 [1:01:17<46:22, 12.82s/it, loss=0.108]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch1_batch300.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  77%|███████▋  | 400/517 [1:21:30<24:43, 12.68s/it, loss=0.0871]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch1_batch400.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  97%|█████████▋| 500/517 [1:41:36<03:22, 11.92s/it, loss=0.109]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch1_batch500.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss Epoch 1: 0.5369\n",
            "Epoch 1 checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch1_end.pth\n",
            "\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  19%|█▉        | 100/517 [00:40<03:20,  2.08it/s, loss=0.0364]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch2_batch100.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  39%|███▊      | 200/517 [01:21<02:29,  2.13it/s, loss=0.0233]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch2_batch200.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  58%|█████▊    | 300/517 [02:02<01:40,  2.17it/s, loss=0.0243]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch2_batch300.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  77%|███████▋  | 400/517 [02:43<00:53,  2.17it/s, loss=0.0531]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch2_batch400.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  97%|█████████▋| 500/517 [03:24<00:08,  2.05it/s, loss=0.00842]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch2_batch500.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss Epoch 2: 0.0739\n",
            "Epoch 2 checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch2_end.pth\n",
            "\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  19%|█▉        | 100/517 [00:41<03:12,  2.16it/s, loss=0.0139]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch3_batch100.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  39%|███▊      | 200/517 [01:21<02:28,  2.13it/s, loss=0.00835]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch3_batch200.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  58%|█████▊    | 300/517 [02:02<01:38,  2.19it/s, loss=0.0533]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch3_batch300.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  77%|███████▋  | 400/517 [02:42<00:54,  2.16it/s, loss=0.00172]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch3_batch400.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  97%|█████████▋| 500/517 [03:23<00:08,  2.12it/s, loss=0.312]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch3_batch500.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss Epoch 3: 0.0428\n",
            "Epoch 3 checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch3_end.pth\n",
            "\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  19%|█▉        | 100/517 [00:40<03:12,  2.17it/s, loss=0.00562]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch4_batch100.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  39%|███▊      | 200/517 [01:21<02:25,  2.18it/s, loss=0.0138]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch4_batch200.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  58%|█████▊    | 300/517 [02:01<01:40,  2.17it/s, loss=0.00345]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch4_batch300.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  77%|███████▋  | 400/517 [02:42<00:53,  2.17it/s, loss=0.00271]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/checkpoint_epoch4_batch400.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  80%|███████▉  | 412/517 [02:47<00:42,  2.45it/s, loss=0.00696]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JK-KYdwyT2Qm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}